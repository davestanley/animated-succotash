{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and load data\n",
    "# Includes\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Setup paths containing utility\n",
    "curr_folder = os.getcwd()\n",
    "sys.path.insert(0, os.path.join(curr_folder,'../app'))\n",
    "\n",
    "# Load the data\n",
    "from utils import load_SQuAD_train\n",
    "arts = load_SQuAD_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 17:49:59 - INFO - allennlp.models.archival -   loading archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\n",
      "01/24/2019 17:49:59 - INFO - allennlp.models.archival -   extracting archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz to temp dir /tmp/tmprq7yiw6o\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   type = default\n",
      "01/24/2019 17:50:04 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmprq7yiw6o/vocabulary.\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'type': 'crf_tagger', 'include_start_end_transitions': False, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'label_encoding': 'BIOUL', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'dropout': 0.5, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.type = crf_tagger\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.crf_tagger.CrfTagger'> from params {'include_start_end_transitions': False, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'label_encoding': 'BIOUL', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'dropout': 0.5, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.type = character_encoding\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.num_embeddings = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'} and extras {}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128} and extras {}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.num_filters = 128\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [3]\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.dropout = 0.0\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 50\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'options_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.options_file\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmprq7yiw6o/fta/model.text_field_embedder.elmo.weight_file\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "01/24/2019 17:50:04 - INFO - allennlp.modules.elmo -   Initializing ELMo\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.type = lstm\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.dropout = 0.5\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.hidden_size = 200\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.input_size = 1202\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.num_layers = 2\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.label_namespace = labels\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.label_encoding = BIOUL\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.include_start_end_transitions = False\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.constrain_crf_decoding = None\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.calculate_span_f1 = None\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.dropout = 0.5\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.verbose_metrics = False\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.regularizer = [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]]\n",
      "01/24/2019 17:50:11 - INFO - allennlp.common.params -   model.regularizer.list.list.type = l2\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      crf._constraint_mask\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      crf.transitions\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1_reverse\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "01/24/2019 17:50:11 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'conll2003'} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.type = conll2003\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.conll2003.Conll2003DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'elmo_characters'} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer from params {} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'characters'} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.type = characters\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.namespace = token_characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras {}\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.tag_label = ner\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.feature_labels = ()\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.coding_scheme = BIOUL\n",
      "01/24/2019 17:50:12 - INFO - allennlp.common.params -   dataset_reader.label_namespace = labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did\tO\n",
      "Uriah\tU-PER\n",
      "honestly\tO\n",
      "think\tO\n",
      "he\tO\n",
      "could\tO\n",
      "beat\tO\n",
      "The\tB-MISC\n",
      "Legend\tI-MISC\n",
      "of\tI-MISC\n",
      "Zelda\tL-MISC\n",
      "in\tO\n",
      "under\tO\n",
      "three\tO\n",
      "hours\tO\n",
      "?\tO\n"
     ]
    }
   ],
   "source": [
    "# Set up and test AllenNLP\n",
    "from allennlp.predictors import Predictor\n",
    "predictor = Predictor.from_path(\"/home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\")\n",
    "results = predictor.predict(sentence=\"Did Uriah honestly think he could beat The Legend of Zelda in under three hours?\")\n",
    "for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "    print(f\"{word}\\t{tag}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name to be a blend of the words gene and chromosome. However, see omics for a more thorough discussion. A few related -ome words already existed—such as biome, rhizome, forming a vocabulary into which genome fits systematically.\n"
     ]
    }
   ],
   "source": [
    "# Choose a paragraph\n",
    "paragraph = arts[15]['paragraphs'][1]['context']\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules specific to this code\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Set up other global variables\n",
    "debug_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nsentences=4\n"
     ]
    }
   ],
   "source": [
    "# Split into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "Nsent = len(sentences)\n",
    "if debug_mode:\n",
    "    print(\"Nsentences=\" + str(Nsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The\tO\n",
      "term\tO\n",
      "was\tO\n",
      "created\tO\n",
      "in\tO\n",
      "1920\tO\n",
      "by\tO\n",
      "Hans\tB-PER\n",
      "Winkler\tL-PER\n",
      ",\tO\n",
      "professor\tO\n",
      "of\tO\n",
      "botany\tO\n",
      "at\tO\n",
      "the\tO\n",
      "University\tB-ORG\n",
      "of\tI-ORG\n",
      "Hamburg\tL-ORG\n",
      ",\tO\n",
      "Germany\tU-LOC\n",
      ".\tO\n",
      "The\tO\n",
      "Oxford\tB-ORG\n",
      "Dictionary\tL-ORG\n",
      "suggests\tO\n",
      "the\tO\n",
      "name\tO\n",
      "to\tO\n",
      "be\tO\n",
      "a\tO\n",
      "blend\tO\n",
      "of\tO\n",
      "the\tO\n",
      "words\tO\n",
      "gene\tO\n",
      "and\tO\n",
      "chromosome\tO\n",
      ".\tO\n",
      "However\tO\n",
      ",\tO\n",
      "see\tO\n",
      "omics\tO\n",
      "for\tO\n",
      "a\tO\n",
      "more\tO\n",
      "thorough\tO\n",
      "discussion\tO\n",
      ".\tO\n",
      "A\tO\n",
      "few\tO\n",
      "related\tO\n",
      "-ome\tO\n",
      "words\tO\n",
      "already\tO\n",
      "existed\tO\n",
      "—\tO\n",
      "such\tO\n",
      "as\tO\n",
      "biome\tO\n",
      ",\tO\n",
      "rhizome\tO\n",
      ",\tO\n",
      "forming\tO\n",
      "a\tO\n",
      "vocabulary\tO\n",
      "into\tO\n",
      "which\tO\n",
      "genome\tO\n",
      "fits\tO\n",
      "systematically\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "# Run it on my test sentence\n",
    "results = predictor.predict(sentence=paragraph)\n",
    "for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "    if debug_mode:\n",
    "        print(f\"{word}\\t{tag}\")\n",
    "    \n",
    "words = results['words']\n",
    "tags = results['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of named entities:[7, 8, 15, 16, 17, 19, 22, 23]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all named entity tags in the tags list\n",
    "def find_inds_of_NE(tags):\n",
    "    # Find list of named entitites\n",
    "    l = [i for i, t in enumerate(tags) if not t == 'O']\n",
    "    return(l)\n",
    "\n",
    "l_NE = find_inds_of_NE(tags)\n",
    "if debug_mode:\n",
    "    print(\"Indices of named entities:\" + str(l_NE))\n",
    "\n",
    "\n",
    "# Choose one at random\n",
    "import random\n",
    "ind = random.choice(l_NE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans ____, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name to be a blend of the words gene and chromosome. However, see omics for a more thorough discussion. A few related -ome words already existed — such as biome, rhizome, forming a vocabulary into which genome fits systematically.\n",
      "Answer: Winkler\n"
     ]
    }
   ],
   "source": [
    "def join_punctuation(seq, characters='.,;?!'):\n",
    "    # For joining lists of words together with correct\n",
    "    # punctuation\n",
    "    characters = set(characters)\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "    yield current\n",
    "\n",
    "\n",
    "# Back up blanked out word and word type\n",
    "removed_word = words[ind]\n",
    "removed_word_tag = words[ind]\n",
    "\n",
    "# Replace chosen word with blank\n",
    "words_new = words.copy()\n",
    "blank_token = '____'\n",
    "words_new[ind] = blank_token\n",
    "\n",
    "# Rebuild the sentence with appropriate punctuation\n",
    "#paragraph_new = ' '.join(words_new).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "paragraph_new = ' '.join(join_punctuation(words_new))\n",
    "\n",
    "# Print this sentence along with the previous sentence together\n",
    "if debug_mode: print(paragraph_new)\n",
    "if debug_mode: print(\"Answer: \" + removed_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, figure out which sentence contains the blank and only present it and the previous two\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# First, figure out the index of the sentence containing the blank\n",
    "sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "curr_word = 0\n",
    "i=0 \n",
    "for sent in sentences_new:\n",
    "    i=i+1\n",
    "    curr_word = curr_word + len(sent.split())\n",
    "    if ind < curr_word:\n",
    "        break\n",
    "\n",
    "ind_sentence_containing_blank = i-1\n",
    "if debug_mode: print(ind_sentence_containing_blank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if debug_mode:\n",
    "#     # This method searches for the token directly - it is slower, but guaranteed to work)\n",
    "#     # Never mind, punctuation messes this method up. No time to fix\n",
    "#     sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "#     i=0\n",
    "#     for sent in sentences_new:\n",
    "#         if blank_token in sent.split():\n",
    "#             break\n",
    "#         i=i+1\n",
    "#     ind_sentence_containing_blank2 = i\n",
    "#     if debug_mode: print(ind_sentence_containing_blank2)\n",
    "#     if not ind_sentence_containing_blank2 == ind_sentence_containing_blank: print('Error occurred')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose a subset of ~2 senteces preceeding the one containing the blank to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford ____ suggests the name to be a blend of the words gene and chromosome.\n"
     ]
    }
   ],
   "source": [
    "def get_two_preceeding_sentences(sentences,ind):\n",
    "    debug_mode = False\n",
    "    sentences_subset = []\n",
    "\n",
    "    # Store current and previous sentences\n",
    "    sentences_subset.append(sentences[ind-2] if ind >= 2 else str(''))\n",
    "    sentences_subset.append(sentences[ind-1] if ind >= 1 else str(''))\n",
    "    sentences_subset.append(sentences[ind])\n",
    "\n",
    "    if debug_mode:\n",
    "        print(\"Current sentence number=\" + str(ind))\n",
    "        print(\"Sentence i-2 = \" + sentences_subset[0])\n",
    "        print(\"Sentence i-1 = \" + sentences_subset[1])\n",
    "        print(\"Sentence i = \" + sentences_subset[2])\n",
    "        \n",
    "    return sentences_subset\n",
    "\n",
    "\n",
    "sentences_subset = get_two_preceeding_sentences(sentences_new,ind_sentence_containing_blank)\n",
    "paragraph_subset = ' '.join(join_punctuation([sent for sent in sentences_subset if sent]))  # Joins only if sentence is non-empty\n",
    "\n",
    "print(paragraph_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge everything into procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all named entity tags in the tags list\n",
    "def find_inds_of_NE(tags):\n",
    "    # Find list of named entitites\n",
    "    l = [i for i, t in enumerate(tags) if not t == 'O']\n",
    "    return(l)\n",
    "\n",
    "def join_punctuation(seq, characters='.,;?!'):\n",
    "    # For joining lists of words together with correct\n",
    "    # punctuation\n",
    "    characters = set(characters)\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "    yield current\n",
    "\n",
    "    \n",
    "def tag_paragraph(paragraph,verbose_mode=False):\n",
    "    from allennlp.predictors import Predictor\n",
    "    \n",
    "\n",
    "    # Set up other global variables\n",
    "    verbose_mode = False\n",
    "\n",
    "\n",
    "    # Run it on my test sentence\n",
    "    predictor = Predictor.from_path(\"/home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\")\n",
    "    results = predictor.predict(sentence=paragraph)\n",
    "    for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "        if verbose_mode:\n",
    "            print(f\"{word}\\t{tag}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Choose a subset of ~2 senteces preceeding the one containing the blank to display\n",
    "def get_two_preceeding_sentences(sentences,ind):\n",
    "    verbose_mode = False\n",
    "    sentences_subset = []\n",
    "\n",
    "    # Store current and previous sentences\n",
    "    sentences_subset.append(sentences[ind-2] if ind >= 2 else str(''))\n",
    "    sentences_subset.append(sentences[ind-1] if ind >= 1 else str(''))\n",
    "    sentences_subset.append(sentences[ind])\n",
    "\n",
    "    if verbose_mode:\n",
    "        print(\"Current sentence number=\" + str(ind))\n",
    "        print(\"Sentence i-2 = \" + sentences_subset[0])\n",
    "        print(\"Sentence i-1 = \" + sentences_subset[1])\n",
    "        print(\"Sentence i = \" + sentences_subset[2])\n",
    "        \n",
    "    return sentences_subset\n",
    "\n",
    "\n",
    "def extract_blanked_out_sentences(results,verbose_mode = False):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    words = results['words']\n",
    "    tags = results['tags']\n",
    "\n",
    "    l_NE = find_inds_of_NE(tags)\n",
    "    if verbose_mode:\n",
    "        print(\"Indices of named entities:\" + str(l_NE))\n",
    "\n",
    "\n",
    "    # Choose one at random\n",
    "    import random\n",
    "    ind = random.choice(l_NE)\n",
    "\n",
    "\n",
    "    # Back up blanked out word and word type\n",
    "    removed_word = words[ind]\n",
    "    removed_word_tag = words[ind]\n",
    "\n",
    "    # Replace chosen word with blank\n",
    "    words_new = words.copy()\n",
    "    blank_token = '____'\n",
    "    words_new[ind] = blank_token\n",
    "\n",
    "    # Rebuild the sentence with appropriate punctuation\n",
    "    #paragraph_new = ' '.join(words_new).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "    paragraph_new = ' '.join(join_punctuation(words_new))\n",
    "\n",
    "    # Print this sentence along with the previous sentence together\n",
    "    if verbose_mode: print(paragraph_new)\n",
    "    if verbose_mode: print(\"Answer: \" + removed_word)\n",
    "\n",
    "\n",
    "\n",
    "    # Finally, figure out which sentence contains the blank and only present it and the previous two\n",
    "    # ===\n",
    "\n",
    "    # First, figure out the index of the sentence containing the blank\n",
    "    sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "    curr_word = 0\n",
    "    i=0 \n",
    "    for sent in sentences_new:\n",
    "        i=i+1\n",
    "        curr_word = curr_word + len(sent.split())\n",
    "        if ind < curr_word:\n",
    "            break\n",
    "\n",
    "    ind_sentence_containing_blank = i-1\n",
    "    if verbose_mode: print(ind_sentence_containing_blank)\n",
    "\n",
    "\n",
    "    # if verbose_mode:\n",
    "    #     # This method searches for the token directly - it is slower, but guaranteed to work)\n",
    "    #     # Never mind, punctuation messes this method up. No time to fix\n",
    "    #     sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "    #     i=0\n",
    "    #     for sent in sentences_new:\n",
    "    #         if blank_token in sent.split():\n",
    "    #             break\n",
    "    #         i=i+1\n",
    "    #     ind_sentence_containing_blank2 = i\n",
    "    #     if verbose_mode: print(ind_sentence_containing_blank2)\n",
    "    #     if not ind_sentence_containing_blank2 == ind_sentence_containing_blank: print('Error occurred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sentences_subset = get_two_preceeding_sentences(sentences_new,ind_sentence_containing_blank)\n",
    "    paragraph_subset = ' '.join(join_punctuation([sent for sent in sentences_subset if sent]))  # Joins only if sentence is non-empty\n",
    "\n",
    "    return paragraph_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name to be a blend of the words gene and chromosome. However, see omics for a more thorough discussion. A few related -ome words already existed—such as biome, rhizome, forming a vocabulary into which genome fits systematically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 18:31:10 - INFO - allennlp.models.archival -   loading archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\n",
      "01/24/2019 18:31:10 - INFO - allennlp.models.archival -   extracting archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz to temp dir /tmp/tmpdj2910ov\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   type = default\n",
      "01/24/2019 18:31:15 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpdj2910ov/vocabulary.\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'include_start_end_transitions': False, 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'dropout': 0.5, 'label_encoding': 'BIOUL', 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'type': 'crf_tagger', 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.options_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.type = crf_tagger\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.crf_tagger.CrfTagger'> from params {'include_start_end_transitions': False, 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'dropout': 0.5, 'label_encoding': 'BIOUL', 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.options_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.options_file'}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.type = character_encoding\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.num_embeddings = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'} and extras {}\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128} and extras {}\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.num_filters = 128\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [3]\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.dropout = 0.0\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 50\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "01/24/2019 18:31:15 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.options_file'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.options_file\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmpdj2910ov/fta/model.text_field_embedder.elmo.weight_file\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "01/24/2019 18:31:16 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "01/24/2019 18:31:16 - INFO - allennlp.modules.elmo -   Initializing ELMo\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.type = lstm\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.dropout = 0.5\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.hidden_size = 200\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.input_size = 1202\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.num_layers = 2\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.label_namespace = labels\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.label_encoding = BIOUL\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.include_start_end_transitions = False\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.constrain_crf_decoding = None\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.calculate_span_f1 = None\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.dropout = 0.5\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.verbose_metrics = False\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.regularizer = [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]]\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   model.regularizer.list.list.type = l2\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      crf._constraint_mask\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      crf.transitions\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1_reverse\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'conll2003'} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.type = conll2003\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.conll2003.Conll2003DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'elmo_characters'} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer from params {} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'characters'} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.type = characters\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.namespace = token_characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras {}\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.tag_label = ner\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.feature_labels = ()\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.coding_scheme = BIOUL\n",
      "01/24/2019 18:31:22 - INFO - allennlp.common.params -   dataset_reader.label_namespace = labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import modules specific to this code\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "# Choose a paragraph\n",
    "paragraph = arts[15]['paragraphs'][1]['context']\n",
    "print(paragraph)\n",
    "\n",
    "# Tag the paragraph\n",
    "results = tag_paragraph(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the sentences\n",
    "paragraph_subset = extract_blanked_out_sentences(results)\n",
    "print(paragraph_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
