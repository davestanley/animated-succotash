{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic setup stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and load data\n",
    "# Includes\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "# Setup paths containing utility\n",
    "curr_folder = os.getcwd()\n",
    "sys.path.insert(0, os.path.join(curr_folder,'../app'))\n",
    "\n",
    "# Load the data\n",
    "from utils import load_SQuAD_train\n",
    "arts = load_SQuAD_train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/29/2019 11:05:34 - INFO - allennlp.models.archival -   loading archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\n",
      "01/29/2019 11:05:34 - INFO - allennlp.models.archival -   extracting archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz to temp dir /tmp/tmpfkj_gkmf\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   type = default\n",
      "01/29/2019 11:05:39 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpfkj_gkmf/vocabulary.\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'include_start_end_transitions': False, 'dropout': 0.5, 'type': 'crf_tagger', 'label_encoding': 'BIOUL', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'elmo': {'do_layer_norm': False, 'dropout': 0, 'type': 'elmo_token_embedder', 'options_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.type = crf_tagger\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.crf_tagger.CrfTagger'> from params {'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'include_start_end_transitions': False, 'dropout': 0.5, 'label_encoding': 'BIOUL', 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'text_field_embedder': {'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'elmo': {'do_layer_norm': False, 'dropout': 0, 'type': 'elmo_token_embedder', 'options_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.weight_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'elmo': {'do_layer_norm': False, 'dropout': 0, 'type': 'elmo_token_embedder', 'options_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.weight_file'}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 50\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.type = character_encoding\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.num_embeddings = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "01/29/2019 11:05:39 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'} and extras {}\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128} and extras {}\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.num_filters = 128\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [3]\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.dropout = 0.0\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'do_layer_norm': False, 'dropout': 0, 'type': 'elmo_token_embedder', 'options_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.options_file', 'weight_file': '/tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.weight_file'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.options_file\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmpfkj_gkmf/fta/model.text_field_embedder.elmo.weight_file\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "01/29/2019 11:05:40 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "01/29/2019 11:05:40 - INFO - allennlp.modules.elmo -   Initializing ELMo\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.type = lstm\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.dropout = 0.5\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.hidden_size = 200\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.input_size = 1202\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.num_layers = 2\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.label_namespace = labels\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.label_encoding = BIOUL\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.include_start_end_transitions = False\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.constrain_crf_decoding = None\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.calculate_span_f1 = None\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.dropout = 0.5\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.verbose_metrics = False\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.regularizer = [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]]\n",
      "01/29/2019 11:05:46 - INFO - allennlp.common.params -   model.regularizer.list.list.type = l2\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      crf._constraint_mask\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      crf.transitions\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1_reverse\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "01/29/2019 11:05:46 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'conll2003'} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.type = conll2003\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.conll2003.Conll2003DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'elmo_characters'} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer from params {} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'characters'} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.type = characters\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras {}\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.tag_label = ner\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.feature_labels = ()\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.coding_scheme = BIOUL\n",
      "01/29/2019 11:05:47 - INFO - allennlp.common.params -   dataset_reader.label_namespace = labels\n"
     ]
    }
   ],
   "source": [
    "# Set up and test AllenNLP\n",
    "from allennlp.predictors import Predictor\n",
    "predictor = Predictor.from_path(\"/home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\")\n",
    "# results = predictor.predict(sentence=\"Did Uriah honestly think he could beat The Legend of Zelda in under three hours?\")\n",
    "# for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "#     print(f\"{word}\\t{tag}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "art = arts[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch_(music)\n"
     ]
    }
   ],
   "source": [
    "print(art['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch is an auditory sensation in which a listener assigns musical tones to relative positions on a musical scale based primarily on their perception of the frequency of vibration. Pitch is closely related to frequency, but the two are not equivalent. Frequency is an objective, scientific attribute that can be measured. Pitch is each person's subjective perception of a sound, which cannot be directly measured. However, this does not necessarily mean that most people won't agree on which notes are higher and lower.\n"
     ]
    }
   ],
   "source": [
    "# Choose a paragraph\n",
    "paragraph = art['paragraphs'][0]['context']\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auditory', 'frequency', 'Pitch']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Candidate blanks\n",
    "[a['text'] for qa in art['paragraphs'][0]['qas'] for a in qa['answers']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules specific to this code\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Set up other global variables\n",
    "debug_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nsentences=5\n"
     ]
    }
   ],
   "source": [
    "# Split into sentences\n",
    "sentences = sent_tokenize(paragraph)\n",
    "Nsent = len(sentences)\n",
    "if debug_mode:\n",
    "    print(\"Nsentences=\" + str(Nsent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pitch\tO\n",
      "is\tO\n",
      "an\tO\n",
      "auditory\tO\n",
      "sensation\tO\n",
      "in\tO\n",
      "which\tO\n",
      "a\tO\n",
      "listener\tO\n",
      "assigns\tO\n",
      "musical\tO\n",
      "tones\tO\n",
      "to\tO\n",
      "relative\tO\n",
      "positions\tO\n",
      "on\tO\n",
      "a\tO\n",
      "musical\tO\n",
      "scale\tO\n",
      "based\tO\n",
      "primarily\tO\n",
      "on\tO\n",
      "their\tO\n",
      "perception\tO\n",
      "of\tO\n",
      "the\tO\n",
      "frequency\tO\n",
      "of\tO\n",
      "vibration\tO\n",
      ".\tO\n",
      "Pitch\tO\n",
      "is\tO\n",
      "closely\tO\n",
      "related\tO\n",
      "to\tO\n",
      "frequency\tO\n",
      ",\tO\n",
      "but\tO\n",
      "the\tO\n",
      "two\tO\n",
      "are\tO\n",
      "not\tO\n",
      "equivalent\tO\n",
      ".\tO\n",
      "Frequency\tO\n",
      "is\tO\n",
      "an\tO\n",
      "objective\tO\n",
      ",\tO\n",
      "scientific\tO\n",
      "attribute\tO\n",
      "that\tO\n",
      "can\tO\n",
      "be\tO\n",
      "measured\tO\n",
      ".\tO\n",
      "Pitch\tO\n",
      "is\tO\n",
      "each\tO\n",
      "person\tO\n",
      "'s\tO\n",
      "subjective\tO\n",
      "perception\tO\n",
      "of\tO\n",
      "a\tO\n",
      "sound\tO\n",
      ",\tO\n",
      "which\tO\n",
      "can\tO\n",
      "not\tO\n",
      "be\tO\n",
      "directly\tO\n",
      "measured\tO\n",
      ".\tO\n",
      "However\tO\n",
      ",\tO\n",
      "this\tO\n",
      "does\tO\n",
      "not\tO\n",
      "necessarily\tO\n",
      "mean\tO\n",
      "that\tO\n",
      "most\tO\n",
      "people\tO\n",
      "wo\tO\n",
      "n't\tO\n",
      "agree\tO\n",
      "on\tO\n",
      "which\tO\n",
      "notes\tO\n",
      "are\tO\n",
      "higher\tO\n",
      "and\tO\n",
      "lower\tO\n",
      ".\tO\n"
     ]
    }
   ],
   "source": [
    "# Run it on my test sentence\n",
    "results = predictor.predict(sentence=paragraph)\n",
    "for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "    if debug_mode:\n",
    "        print(f\"{word}\\t{tag}\")\n",
    "    \n",
    "words = results['words']\n",
    "tags = results['tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices of named entities:[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all named entity tags in the tags list\n",
    "def find_inds_of_NE(tags):\n",
    "    # Find list of named entitites\n",
    "    l = [i for i, t in enumerate(tags) if not t == 'O']\n",
    "    return(l)\n",
    "\n",
    "l_NE = find_inds_of_NE(tags)\n",
    "if debug_mode:\n",
    "    print(\"Indices of named entities:\" + str(l_NE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans Winkler, professor of botany at the ____ of Hamburg, Germany. The Oxford Dictionary suggests the name to be a blend of the words gene and chromosome. However, see omics for a more thorough discussion. A few related -ome words already existed — such as biome, rhizome, forming a vocabulary into which genome fits systematically.\n",
      "Answer: University\n"
     ]
    }
   ],
   "source": [
    "def join_punctuation(seq, characters='.,;?!'):\n",
    "    # For joining lists of words together with correct\n",
    "    # punctuation\n",
    "    characters = set(characters)\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "    yield current\n",
    "\n",
    "\n",
    "# Choose one at random\n",
    "import random\n",
    "ind = random.choice(l_NE)\n",
    "\n",
    "# Back up blanked out word and word type\n",
    "removed_word = words[ind]\n",
    "removed_word_tag = words[ind]\n",
    "\n",
    "# Replace chosen word with blank\n",
    "words_new = words.copy()\n",
    "blank_token = '____'\n",
    "words_new[ind] = blank_token\n",
    "\n",
    "# Rebuild the sentence with appropriate punctuation\n",
    "#paragraph_new = ' '.join(words_new).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "paragraph_new = ' '.join(join_punctuation(words_new))\n",
    "\n",
    "# Print this sentence along with the previous sentence together\n",
    "if debug_mode: print(paragraph_new)\n",
    "if debug_mode: print(\"Answer: \" + removed_word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, figure out which sentence contains the blank and only present it and the previous two\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# First, figure out the index of the sentence containing the blank\n",
    "sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "curr_word = 0\n",
    "i=0 \n",
    "for sent in sentences_new:\n",
    "    i=i+1\n",
    "    curr_word = curr_word + len(sent.split())\n",
    "    if ind < curr_word:\n",
    "        break\n",
    "\n",
    "ind_sentence_containing_blank = i-1\n",
    "if debug_mode: print(ind_sentence_containing_blank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if debug_mode:\n",
    "#     # This method searches for the token directly - it is slower, but guaranteed to work)\n",
    "#     # Never mind, punctuation messes this method up. No time to fix\n",
    "#     sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "#     i=0\n",
    "#     for sent in sentences_new:\n",
    "#         if blank_token in sent.split():\n",
    "#             break\n",
    "#         i=i+1\n",
    "#     ind_sentence_containing_blank2 = i\n",
    "#     if debug_mode: print(ind_sentence_containing_blank2)\n",
    "#     if not ind_sentence_containing_blank2 == ind_sentence_containing_blank: print('Error occurred')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose a subset of ~2 senteces preceeding the one containing the blank to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford ____ suggests the name to be a blend of the words gene and chromosome.\n"
     ]
    }
   ],
   "source": [
    "def get_two_preceeding_sentences(sentences,ind):\n",
    "    debug_mode = False\n",
    "    sentences_subset = []\n",
    "\n",
    "    # Store current and previous sentences\n",
    "    sentences_subset.append(sentences[ind-2] if ind >= 2 else str(''))\n",
    "    sentences_subset.append(sentences[ind-1] if ind >= 1 else str(''))\n",
    "    sentences_subset.append(sentences[ind])\n",
    "\n",
    "    if debug_mode:\n",
    "        print(\"Current sentence number=\" + str(ind))\n",
    "        print(\"Sentence i-2 = \" + sentences_subset[0])\n",
    "        print(\"Sentence i-1 = \" + sentences_subset[1])\n",
    "        print(\"Sentence i = \" + sentences_subset[2])\n",
    "        \n",
    "    return sentences_subset\n",
    "\n",
    "\n",
    "sentences_subset = get_two_preceeding_sentences(sentences_new,ind_sentence_containing_blank)\n",
    "paragraph_subset = ' '.join(join_punctuation([sent for sent in sentences_subset if sent]))  # Joins only if sentence is non-empty\n",
    "\n",
    "print(paragraph_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge everything into procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### NLP Functions #########################\n",
    "\n",
    "def find_inds_of_NE(tags):\n",
    "    # Get indices of all named entity tags in the tags list\n",
    "    l = [i for i, t in enumerate(tags) if not t == 'O']\n",
    "    return(l)\n",
    "\n",
    "def join_punctuation(seq, characters='.,;?!'):\n",
    "    # For joining lists of words together with correct\n",
    "    # punctuation spacings\n",
    "    characters = set(characters)\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "    yield current\n",
    "\n",
    "\n",
    "def tag_paragraph_NER(paragraph,verbose_mode=False):\n",
    "    # Use AllenNLP to run NER on paragraph\n",
    "    from allennlp.predictors import Predictor\n",
    "\n",
    "\n",
    "    # Set up other global variables\n",
    "    verbose_mode = False\n",
    "\n",
    "\n",
    "    # Run it on my test sentence\n",
    "    predictor = Predictor.from_path(\"/home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\")\n",
    "    results = predictor.predict(sentence=paragraph)\n",
    "    for word, tag in zip(results[\"words\"], results[\"tags\"]):\n",
    "        if verbose_mode:\n",
    "            print(f\"{word}\\t{tag}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_two_preceeding_sentences(sentences,ind,verbose_mode=False):\n",
    "    # Returns a subset sentences:\n",
    "        # sentences[ind] and sentences[ind-1] and sentences[ind-2]\n",
    "        # only if they exist\n",
    "    sentences_subset = []\n",
    "\n",
    "    # Store current and previous sentences\n",
    "    sentences_subset.append(sentences[ind-2] if ind >= 2 else str(''))\n",
    "    sentences_subset.append(sentences[ind-1] if ind >= 1 else str(''))\n",
    "    sentences_subset.append(sentences[ind])\n",
    "\n",
    "    if verbose_mode:\n",
    "        print(\"Current sentence number=\" + str(ind))\n",
    "        print(\"Sentence i-2 = \" + sentences_subset[0])\n",
    "        print(\"Sentence i-1 = \" + sentences_subset[1])\n",
    "        print(\"Sentence i = \" + sentences_subset[2])\n",
    "\n",
    "    return sentences_subset\n",
    "\n",
    "\n",
    "def extract_blanked_out_sentences(results,verbose_mode = False):\n",
    "    # From results word/token list, blank out a random word, and then return\n",
    "    # the sentences containing that blanked out word, plus a few preceding\n",
    "    # centences for context.\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    words = results['words']\n",
    "    tags = results['tags']\n",
    "\n",
    "    l_NE = find_inds_of_NE(tags)\n",
    "    if verbose_mode:\n",
    "        print(\"Indices of named entities:\" + str(l_NE))\n",
    "\n",
    "\n",
    "    # Choose one at random\n",
    "    import random\n",
    "    ind = random.choice(l_NE)\n",
    "\n",
    "\n",
    "    # Back up blanked out word and word type\n",
    "    removed_word = words[ind]\n",
    "    removed_word_tag = words[ind]\n",
    "\n",
    "    # Replace chosen word with blank\n",
    "    words_new = words.copy()\n",
    "    blank_token = '____'\n",
    "    words_new[ind] = blank_token\n",
    "\n",
    "    # Rebuild the sentence with appropriate punctuation\n",
    "    #paragraph_new = ' '.join(words_new).replace(\" ,\", \",\").replace(\" .\", \".\")\n",
    "    paragraph_new = ' '.join(join_punctuation(words_new))\n",
    "\n",
    "    # Print this sentence along with the previous sentence together\n",
    "    if verbose_mode: print(paragraph_new)\n",
    "    if verbose_mode: print(\"Answer: \" + removed_word)\n",
    "\n",
    "\n",
    "\n",
    "    # Finally, figure out which sentence contains the blank and only present it and the previous two\n",
    "    # ===\n",
    "\n",
    "    # First, figure out the index of the sentence containing the blank\n",
    "    sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "    curr_word = 0\n",
    "    i=0\n",
    "    for sent in sentences_new:\n",
    "        i=i+1\n",
    "        curr_word = curr_word + len(sent.split())\n",
    "        if ind < curr_word:\n",
    "            break\n",
    "\n",
    "    ind_sentence_containing_blank = i-1\n",
    "    if verbose_mode: print(ind_sentence_containing_blank)\n",
    "\n",
    "\n",
    "    # if verbose_mode:\n",
    "    #     # This method searches for the token directly - it is slower, but guaranteed to work)\n",
    "    #     # Never mind, punctuation messes this method up. No time to fix\n",
    "    #     sentences_new = sent_tokenize(paragraph_new)\n",
    "\n",
    "\n",
    "    #     i=0\n",
    "    #     for sent in sentences_new:\n",
    "    #         if blank_token in sent.split():\n",
    "    #             break\n",
    "    #         i=i+1\n",
    "    #     ind_sentence_containing_blank2 = i\n",
    "    #     if verbose_mode: print(ind_sentence_containing_blank2)\n",
    "    #     if not ind_sentence_containing_blank2 == ind_sentence_containing_blank: print('Error occurred')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sentences_subset = get_two_preceeding_sentences(sentences_new,ind_sentence_containing_blank)\n",
    "    paragraph_subset = ' '.join(join_punctuation([sent for sent in sentences_subset if sent]))  # Joins only if sentence is non-empty\n",
    "\n",
    "    text_blanks = dict()\n",
    "    text_blanks['text'] = paragraph_subset\n",
    "    text_blanks['removed_word'] = removed_word\n",
    "    text_blanks['removed_word_tag'] = removed_word_tag\n",
    "\n",
    "    return text_blanks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/25/2019 03:12:48 - INFO - allennlp.models.archival -   loading archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz\n",
      "01/25/2019 03:12:48 - INFO - allennlp.models.archival -   extracting archive file /home/davestanley/src/allennlp/ner-model-2018.12.18.tar.gz to temp dir /tmp/tmpklva6x7m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The Oxford Dictionary suggests the name to be a blend of the words gene and chromosome. However, see omics for a more thorough discussion. A few related -ome words already existed—such as biome, rhizome, forming a vocabulary into which genome fits systematically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/25/2019 03:12:53 - INFO - allennlp.common.params -   type = default\n",
      "01/25/2019 03:12:53 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpklva6x7m/vocabulary.\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'include_start_end_transitions': False, 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'dropout': 0.5, 'label_encoding': 'BIOUL', 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'type': 'crf_tagger', 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.options_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.type = crf_tagger\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.crf_tagger.CrfTagger'> from params {'include_start_end_transitions': False, 'regularizer': [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]], 'dropout': 0.5, 'label_encoding': 'BIOUL', 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'}, 'text_field_embedder': {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.options_file'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_characters': {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'}, 'elmo': {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.options_file'}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 16}, 'encoder': {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.type = character_encoding\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.num_embeddings = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.vocab_namespace = token_characters\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.embedding_dim = 16\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.pretrained_file = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.projection_dim = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.trainable = True\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.padding_index = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.max_norm = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.norm_type = 2.0\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.scale_grad_by_freq = False\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.embedding.sparse = False\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128, 'type': 'cnn'} and extras {}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.type = cnn\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'conv_layer_activation': 'relu', 'embedding_dim': 16, 'ngram_filter_sizes': [3], 'num_filters': 128} and extras {}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.embedding_dim = 16\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.num_filters = 128\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.ngram_filter_sizes = [3]\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.conv_layer_activation = relu\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.encoder.output_dim = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.token_characters.dropout = 0.0\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': True, 'type': 'embedding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.type = embedding\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.num_embeddings = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.vocab_namespace = tokens\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.embedding_dim = 50\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.pretrained_file = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.projection_dim = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.trainable = True\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.padding_index = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.max_norm = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.norm_type = 2.0\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.scale_grad_by_freq = False\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.tokens.sparse = False\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'dropout': 0, 'type': 'elmo_token_embedder', 'do_layer_norm': False, 'weight_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.weight_file', 'options_file': '/tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.options_file'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.type = elmo_token_embedder\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.options_file = /tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.options_file\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.weight_file = /tmp/tmpklva6x7m/fta/model.text_field_embedder.elmo.weight_file\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.requires_grad = False\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.do_layer_norm = False\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.dropout = 0\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.namespace_to_cache = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.projection_dim = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.common.params -   model.text_field_embedder.elmo.scalar_mix_parameters = None\n",
      "01/25/2019 03:12:54 - INFO - allennlp.modules.elmo -   Initializing ELMo\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 200, 'input_size': 1202, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 26871 || token_characters, Size: 87 || labels, Size: 17 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.type = lstm\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.dropout = 0.5\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.hidden_size = 200\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.input_size = 1202\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.num_layers = 2\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.label_namespace = labels\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.label_encoding = BIOUL\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.include_start_end_transitions = False\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.constrain_crf_decoding = None\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.calculate_span_f1 = None\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.dropout = 0.5\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.verbose_metrics = False\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.regularizer = [['scalar_parameters', {'alpha': 0.1, 'type': 'l2'}]]\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   model.regularizer.list.list.type = l2\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      crf._constraint_mask\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      crf.transitions\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1_reverse\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.input_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_0.state_projection.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.input_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.backward_layer_1.state_projection.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.input_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_0.state_projection.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.input_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_linearity.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._elmo_lstm.forward_layer_1.state_projection.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._char_embedding_weights\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.0.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._highways._layers.1.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder._projection.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_0.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_1.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_2.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_3.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_4.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_5.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo._elmo_lstm._token_embedder.char_conv_6.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.gamma\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.0\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.1\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_elmo._elmo.scalar_mix_0.scalar_parameters.2\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._embedding._module.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'conll2003'} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.type = conll2003\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.conll2003.Conll2003DatasetReader'> from params {'coding_scheme': 'BIOUL', 'tag_label': 'ner', 'token_indexers': {'elmo': {'type': 'elmo_characters'}, 'token_characters': {'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'elmo_characters'} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.type = elmo_characters\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer from params {} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.elmo.namespace = elmo_characters\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'type': 'characters'} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.type = characters\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.namespace = token_characters\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.start_tokens = None\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.end_tokens = None\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.token_characters.min_padding_length = 0\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras {}\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.tag_label = ner\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.feature_labels = ()\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.coding_scheme = BIOUL\n",
      "01/25/2019 03:13:01 - INFO - allennlp.common.params -   dataset_reader.label_namespace = labels\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose a paragraph\n",
    "paragraph = arts[15]['paragraphs'][1]['context']\n",
    "print(paragraph)\n",
    "\n",
    "# Tag the paragraph\n",
    "results = tag_paragraph_NER(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The term was created in 1920 by Hans Winkler, professor of botany at the University of Hamburg, Germany. The ____ Dictionary suggests the name to be a blend of the words gene and chromosome.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the sentences\n",
    "text_blanks = extract_blanked_out_sentences(results)\n",
    "\n",
    "blanked_sentence = text_blanks['text']\n",
    "removed_word = text_blanks['removed_word']\n",
    "removed_word_tag = text_blanks['removed_word_tag']\n",
    "\n",
    "\n",
    "print(blanked_sentence)\n",
    "print('Answer: ' + removed_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
