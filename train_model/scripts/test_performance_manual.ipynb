{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up and load data\n",
    "# Includes\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Setup paths containing utility\n",
    "curr_folder = os.getcwd()\n",
    "sys.path.insert(0, os.path.join(curr_folder,'../../../app'))\n",
    "\n",
    "# Utils imports for loading data\n",
    "from utils import save_data, load_data, exists_datafolder\n",
    "from utils import load_SQuAD_train, load_SQuAD_dev\n",
    "from utils import get_foldername\n",
    "from utils_NLP import text2sentences,words2words_blanked,words2words_hashblank,words2answers\n",
    "from utils_NLP import words2text\n",
    "from utils_SQuAD import OR_arts_paragraph_fields,merge_arts_paragraph_fields\n",
    "from utils_NLP import allenNLP_classify_blanks,allenNLP_classify_blanks_fromResults\n",
    "\n",
    "# Plotting includes\n",
    "from utils_EDAplots import plotbar_train_dev,plothist_train_dev,plotbar_train_dev2,plothist_train_dev2\n",
    "\n",
    "# Stats saving stuff\n",
    "from utils_EDA import calcstats_train_dev\n",
    "\n",
    "# AllenNLP stuff\n",
    "from allennlp.predictors import Predictor\n",
    "\n",
    "# Include custom AllenNLP\n",
    "import myallennlp\n",
    "from myallennlp import *\n",
    "from myallennlp.models.simple_tagger2 import SimpleTagger2\n",
    "from myallennlp.dataset_readers import sequence_tagging2\n",
    "from myallennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "\n",
    "# Import fig stuff\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option for merging NER data into combined model\n",
    "merge_in_NER_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/davestanley/Dropbox/git/mindpocket/train_model/articles_batch5.0/entropy_90_10_model6e0c37\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually test the predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/25/2019 23:56:32 - INFO - allennlp.models.archival -   loading archive file ./model.tar.gz\n",
      "02/25/2019 23:56:32 - INFO - allennlp.models.archival -   extracting archive file ./model.tar.gz to temp dir /tmp/tmpkipg_by4\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   type = default\n",
      "02/25/2019 23:56:32 - INFO - allennlp.data.vocabulary -   Loading token dictionary from /tmp/tmpkipg_by4/vocabulary.\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.models.model.Model'> from params {'Ntags0': 90, 'Ntags1': 10, 'do_crossentropy_weighting': True, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 72, 'input_size': 72, 'num_layers': 2, 'type': 'lstm'}, 'text_field_embedder': {'token_embedders': {'dependency_label': {'embedding_dim': 10, 'type': 'embedding', 'vocab_namespace': 'dependencies'}, 'ner_tag': {'embedding_dim': 7, 'type': 'embedding', 'vocab_namespace': 'ner'}, 'pos_tag': {'embedding_dim': 5, 'type': 'embedding', 'vocab_namespace': 'pos'}, 'tokens': {'embedding_dim': 50, 'trainable': False, 'type': 'embedding'}}}, 'type': 'simple_tagger2'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.type = simple_tagger2\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'myallennlp.models.simple_tagger2.SimpleTagger2'> from params {'Ntags0': 90, 'Ntags1': 10, 'do_crossentropy_weighting': True, 'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 72, 'input_size': 72, 'num_layers': 2, 'type': 'lstm'}, 'text_field_embedder': {'token_embedders': {'dependency_label': {'embedding_dim': 10, 'type': 'embedding', 'vocab_namespace': 'dependencies'}, 'ner_tag': {'embedding_dim': 7, 'type': 'embedding', 'vocab_namespace': 'ner'}, 'pos_tag': {'embedding_dim': 5, 'type': 'embedding', 'vocab_namespace': 'pos'}, 'tokens': {'embedding_dim': 50, 'trainable': False, 'type': 'embedding'}}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'dependency_label': {'embedding_dim': 10, 'type': 'embedding', 'vocab_namespace': 'dependencies'}, 'ner_tag': {'embedding_dim': 7, 'type': 'embedding', 'vocab_namespace': 'ner'}, 'pos_tag': {'embedding_dim': 5, 'type': 'embedding', 'vocab_namespace': 'pos'}, 'tokens': {'embedding_dim': 50, 'trainable': False, 'type': 'embedding'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.type = basic\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.embedder_to_indexer_map = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.allow_unmatched_keys = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 10, 'type': 'embedding', 'vocab_namespace': 'dependencies'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.type = embedding\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.num_embeddings = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.vocab_namespace = dependencies\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.embedding_dim = 10\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.pretrained_file = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.projection_dim = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.trainable = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.padding_index = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.max_norm = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.norm_type = 2.0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.scale_grad_by_freq = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.dependency_label.sparse = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 7, 'type': 'embedding', 'vocab_namespace': 'ner'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.type = embedding\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.num_embeddings = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.vocab_namespace = ner\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.embedding_dim = 7\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.pretrained_file = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.projection_dim = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.trainable = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.padding_index = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.max_norm = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.norm_type = 2.0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.scale_grad_by_freq = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.ner_tag.sparse = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 5, 'type': 'embedding', 'vocab_namespace': 'pos'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.type = embedding\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.num_embeddings = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.vocab_namespace = pos\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.embedding_dim = 5\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.pretrained_file = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.projection_dim = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.trainable = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.padding_index = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.max_norm = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.norm_type = 2.0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.scale_grad_by_freq = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.pos_tag.sparse = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'trainable': False, 'type': 'embedding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.type = embedding\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.num_embeddings = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.embedding_dim = 50\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.pretrained_file = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.projection_dim = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.trainable = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.padding_index = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.max_norm = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.norm_type = 2.0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.text_field_embedder.token_embedders.tokens.sparse = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 72, 'input_size': 72, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 88725 || ner, Size: 21 || dependencies, Size: 47 || pos, Size: 17 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.type = lstm\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.stateful = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   CURRENTLY DEFINED PARAMETERS: \n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.bidirectional = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.dropout = 0.5\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.hidden_size = 72\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.input_size = 72\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.num_layers = 2\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.encoder.batch_first = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.do_crossentropy_weighting = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.Ntags0 = 90\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   model.Ntags1 = 10\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -   Initializing parameters\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -   Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l0_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_hh_l1_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l0_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.bias_ih_l1_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l0_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_hh_l1_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l0_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      encoder._module.weight_ih_l1_reverse\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.bias\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      tag_projection_layer._module.weight\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_dependency_label.weight\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_ner_tag.weight\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_pos_tag.weight\n",
      "02/25/2019 23:56:32 - INFO - allennlp.nn.initializers -      text_field_embedder.token_embedder_tokens.weight\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'mytokenizer': {'type': 'word', 'word_splitter': {'ner': True, 'parse': True, 'pos_tags': True, 'type': 'whitespacy'}}, 'token_indexers': {'dependency_label': {'namespace': 'dependencies', 'type': 'dependency_label'}, 'ner_tag': {'namespace': 'ner', 'type': 'ner_tag'}, 'pos_tag': {'coarse_tags': True, 'namespace': 'pos', 'type': 'pos_tag'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'sequence_tagging2', 'word_tag_delimiter': '//'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.type = sequence_tagging2\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'myallennlp.dataset_readers.sequence_tagging2.SequenceTaggingDatasetReader'> from params {'mytokenizer': {'type': 'word', 'word_splitter': {'ner': True, 'parse': True, 'pos_tags': True, 'type': 'whitespacy'}}, 'token_indexers': {'dependency_label': {'namespace': 'dependencies', 'type': 'dependency_label'}, 'ner_tag': {'namespace': 'ner', 'type': 'ner_tag'}, 'pos_tag': {'coarse_tags': True, 'namespace': 'pos', 'type': 'pos_tag'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'word_tag_delimiter': '//'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.word_tag_delimiter = //\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_delimiter = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'namespace': 'dependencies', 'type': 'dependency_label'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.dependency_label.type = dependency_label\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.dep_label_indexer.DepLabelIndexer from params {'namespace': 'dependencies'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.dependency_label.namespace = dependencies\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'namespace': 'ner', 'type': 'ner_tag'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.ner_tag.type = ner_tag\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.ner_tag_indexer.NerTagIndexer from params {'namespace': 'ner'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.ner_tag.namespace = ner\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'coarse_tags': True, 'namespace': 'pos', 'type': 'pos_tag'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.pos_tag.type = pos_tag\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.pos_tag_indexer.PosTagIndexer from params {'coarse_tags': True, 'namespace': 'pos'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.pos_tag.namespace = pos\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.pos_tag.coarse_tags = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.type = single_id\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.namespace = tokens\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.lowercase_tokens = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.start_tokens = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.token_indexers.tokens.end_tokens = None\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.lazy = False\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.tokenizer.Tokenizer'> from params {'type': 'word', 'word_splitter': {'ner': True, 'parse': True, 'pos_tags': True, 'type': 'whitespacy'}} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.type = word\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_tokenizer.WordTokenizer'> from params {'word_splitter': {'ner': True, 'parse': True, 'pos_tags': True, 'type': 'whitespacy'}} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'allennlp.data.tokenizers.word_splitter.WordSplitter'> from params {'ner': True, 'parse': True, 'pos_tags': True, 'type': 'whitespacy'} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.word_splitter.type = whitespacy\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.from_params -   instantiating class <class 'myallennlp.data.tokenizers.word_splitter.SpacyWordSplitter'> from params {'ner': True, 'parse': True, 'pos_tags': True} and extras {}\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.word_splitter.language = en_core_web_sm\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.word_splitter.pos_tags = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.word_splitter.parse = True\n",
      "02/25/2019 23:56:32 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.word_splitter.ner = True\n",
      "02/25/2019 23:56:33 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.start_tokens = None\n",
      "02/25/2019 23:56:33 - INFO - allennlp.common.params -   dataset_reader.mytokenizer.end_tokens = None\n",
      "02/25/2019 23:56:33 - INFO - allennlp.common.params -   dataset_reader.use_spacy_directly = False\n"
     ]
    }
   ],
   "source": [
    "# Set up AllenNLP\n",
    "currmodel = os.path.join('.','model.tar.gz')\n",
    "predictor = Predictor.from_path(currmodel,predictor_name='sentence-tagger');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['___Pitch___', 'is', 'an', '___auditory___', 'sensation', 'based', 'on', 'the', 'frequency', 'of', '___vibration___', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Pitch is an auditory sensation based on the frequency of vibration.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_hashblank(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', '___Normans___', 'are', 'from', 'northern', '___France___', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='The Normans are from northern France.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_hashblank(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['___The___', '___brain___', 'is', 'an', 'organ', 'that', 'serves', 'as', 'the', 'center', 'of', 'the', 'nervous', 'system', 'in', 'all', '___vertebrate___', 'and', 'most', 'invertebrate', 'animals', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='The brain is an organ that serves as the center of the nervous system in all vertebrate and most invertebrate animals.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_hashblank(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', ',', 'the', 'function', 'of', 'the', '______', 'is', 'to', 'exert', '______', 'control', 'over', 'the', 'other', 'organs', 'of', 'the', 'body', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Physiologically, the function of the brain is to exert centralized control over the other organs of the body.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Black', 'Death', 'ravaged', 'Europe', 'for', 'three', 'years', 'before', 'it', 'continued', 'on', 'into', '______', ',', 'where', 'the', 'disease', 'was', 'present', 'somewhere', 'in', 'the', 'country', '______', '______', 'between', '______', 'to', '______', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='The Black Death ravaged Europe for three years before it continued on into Russia, where the disease was present somewhere in the country 25 times between 1350 to 1490.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Black', 'Death', 'ravaged', 'Europe', 'for', 'three', 'years', 'before', 'it', 'continued', 'on', 'into', '______', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='The Black Death ravaged Europe for three years before it continued on into Russia.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'continued', 'on', 'into', '______', ',', 'which', 'is', 'a', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='I continued on into Russia, which is a country.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'continued', 'on', 'into', '______', ',', 'which', 'is', 'a', 'food', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='I continued on into Russia, which is a food.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'the', 'capital', 'of', '______', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Moscow is the capital of Russia.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'the', 'capital', 'of', '______', '.']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='London is the capital of England.');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'the', 'capital', 'of', '______']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Berlin is the capital of England');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'a', 'type', 'of', 'disease']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Plague is a type of disease');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pull out sample paragraph\n",
    "# p = arts[ind_ex_dev]['paragraphs'][0]\n",
    "\n",
    "# # # Print all AllenNLP classifications\n",
    "# # print([(a,b) for a,b in zip(p['allenNER']['words'].split(), p['allenNER']['tags'].split())])\n",
    "\n",
    "# # AllenNLP results\n",
    "# words = p['allenNER']['words'].split()\n",
    "# # tags = p['allenNER']['tags'].split()\n",
    "# # tags = [not t == '0' for t in tags]   # Convert to binary\n",
    "# tags = p['blank_classified_allen']\n",
    "\n",
    "# # Ground truth\n",
    "# blank_classification = p['blank_classification']\n",
    "\n",
    "\n",
    "# words_blanked_ground_truth = words2words_blanked(words,blank_classification)\n",
    "# words_blanked_allen = words2words_blanked(words,tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', '______']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Tea is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', '______']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Pastry is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', 'crumpets']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Cake is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beef', 'is', 'eaten', 'with', 'crumpets']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Beef is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', 'crumpets']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Corn is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', 'crumpets']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Milk is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', 'crumpets']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Juice is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', '______']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='England is eaten with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', '______', 'with', 'with', 'with', 'with', 'with', '______']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Tea is eaten with with with with with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', 'eaten', 'with', 'with', 'with', 'with', 'with', '______']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='England is eaten with with with with with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Beef', 'is', '______', 'with', 'with', 'with', 'with', 'with', 'crumpets']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Beef is eaten with with with with with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['______', 'is', '______', 'with', 'with', 'with', 'with', 'with', 'crumpets']\n"
     ]
    }
   ],
   "source": [
    "results = predictor.predict(sentence='Corn is eaten with with with with with crumpets');\n",
    "words = results['words']\n",
    "tags = allenNLP_classify_blanks_fromResults(results,'0')\n",
    "out = words_blanked_allen = words2words_blanked(words,tags)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:allennlp]",
   "language": "python",
   "name": "conda-env-allennlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
