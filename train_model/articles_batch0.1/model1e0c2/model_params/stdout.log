2019-02-07 19:10:10,723 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'sequence_tagging', 'word_tag_delimiter': '//'} and extras {}
2019-02-07 19:10:10,723 - INFO - allennlp.common.params - dataset_reader.type = sequence_tagging
2019-02-07 19:10:10,724 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.sequence_tagging.SequenceTaggingDatasetReader'> from params {'token_indexers': {'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'word_tag_delimiter': '//'} and extras {}
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.word_tag_delimiter = //
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.token_delimiter = None
2019-02-07 19:10:10,724 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras {}
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2019-02-07 19:10:10,724 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras {}
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - dataset_reader.lazy = False
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - validation_dataset_reader = None
2019-02-07 19:10:10,724 - INFO - allennlp.common.params - train_data_path = ../data/allenTrain.txt
2019-02-07 19:10:10,725 - INFO - allennlp.commands.train - Reading training data from ../data/allenTrain.txt
2019-02-07 19:10:10,725 - INFO - allennlp.data.dataset_readers.sequence_tagging - Reading instances from lines in file at: ../data/allenTrain.txt
2019-02-07 19:10:10,731 - INFO - allennlp.common.params - validation_data_path = ../data/allenDev.txt
2019-02-07 19:10:10,731 - INFO - allennlp.commands.train - Reading validation data from ../data/allenDev.txt
2019-02-07 19:10:10,731 - INFO - allennlp.data.dataset_readers.sequence_tagging - Reading instances from lines in file at: ../data/allenDev.txt
2019-02-07 19:10:10,736 - INFO - allennlp.common.params - test_data_path = None
2019-02-07 19:10:10,736 - INFO - allennlp.commands.train - From dataset instances, validation, train will be considered for vocabulary creation.
2019-02-07 19:10:10,736 - INFO - allennlp.common.params - vocabulary.type = None
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.extend = False
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.directory_path = None
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.min_count = None
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = False
2019-02-07 19:10:10,737 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None
2019-02-07 19:10:10,737 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
2019-02-07 19:10:10,743 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 50, 'input_size': 50, 'num_layers': 2, 'type': 'lstm'}, 'text_field_embedder': {'token_embedders': {'tokens': {'embedding_dim': 50, 'pretrained_file': '~/src/allennlp/glove/glove.6B.50d.txt.gz', 'trainable': False, 'type': 'embedding'}}}, 'type': 'simple_tagger2'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 1147 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}
2019-02-07 19:10:10,743 - INFO - allennlp.common.params - model.type = simple_tagger2
2019-02-07 19:10:10,743 - INFO - allennlp.common.from_params - instantiating class <class 'myallennlp.models.simple_tagger2.SimpleTagger2'> from params {'encoder': {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 50, 'input_size': 50, 'num_layers': 2, 'type': 'lstm'}, 'text_field_embedder': {'token_embedders': {'tokens': {'embedding_dim': 50, 'pretrained_file': '~/src/allennlp/glove/glove.6B.50d.txt.gz', 'trainable': False, 'type': 'embedding'}}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 1147 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}
2019-02-07 19:10:10,743 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'token_embedders': {'tokens': {'embedding_dim': 50, 'pretrained_file': '~/src/allennlp/glove/glove.6B.50d.txt.gz', 'trainable': False, 'type': 'embedding'}}} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 1147 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.embedder_to_indexer_map = None
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = False
2019-02-07 19:10:10,744 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 50, 'pretrained_file': '~/src/allennlp/glove/glove.6B.50d.txt.gz', 'trainable': False, 'type': 'embedding'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 1147 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 50
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = ~/src/allennlp/glove/glove.6B.50d.txt.gz
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = False
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2019-02-07 19:10:10,744 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2019-02-07 19:10:10,745 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2019-02-07 19:10:10,745 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2019-02-07 19:10:10,745 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file
2019-02-07 19:10:12,462 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer
2019-02-07 19:10:12,468 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 1119 out of 1147 tokens
2019-02-07 19:10:12,469 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'bidirectional': True, 'dropout': 0.5, 'hidden_size': 50, 'input_size': 50, 'num_layers': 2, 'type': 'lstm'} and extras {'vocab': Vocabulary with namespaces:  tokens, Size: 1147 || labels, Size: 2 || Non Padded Namespaces: {'*tags', '*labels'}}
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.type = lstm
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.batch_first = True
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.stateful = False
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: 
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.bidirectional = True
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.dropout = 0.5
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.hidden_size = 50
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.input_size = 50
2019-02-07 19:10:12,469 - INFO - allennlp.common.params - model.encoder.num_layers = 2
2019-02-07 19:10:12,470 - INFO - allennlp.common.params - model.encoder.batch_first = True
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers - Initializing parameters
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l0
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l0_reverse
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l1
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers -    encoder._module.bias_hh_l1_reverse
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l0
2019-02-07 19:10:12,471 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l0_reverse
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l1
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.bias_ih_l1_reverse
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l0
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l0_reverse
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l1
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_hh_l1_reverse
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l0
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l0_reverse
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l1
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    encoder._module.weight_ih_l1_reverse
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    tag_projection_layer._module.bias
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    tag_projection_layer._module.weight
2019-02-07 19:10:12,472 - INFO - allennlp.nn.initializers -    text_field_embedder.token_embedder_tokens.weight
2019-02-07 19:10:12,476 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 32, 'type': 'basic'} and extras {}
2019-02-07 19:10:12,476 - INFO - allennlp.common.params - iterator.type = basic
2019-02-07 19:10:12,477 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.basic_iterator.BasicIterator'> from params {'batch_size': 32} and extras {}
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - iterator.batch_size = 32
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - iterator.instances_per_epoch = None
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - iterator.max_instances_in_memory = None
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - iterator.cache_instances = False
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - iterator.track_epoch = False
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - validation_iterator = None
2019-02-07 19:10:12,477 - INFO - allennlp.common.params - trainer.no_grad = ()
2019-02-07 19:10:12,477 - INFO - allennlp.commands.train - Following parameters are Frozen  (without gradient):
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - text_field_embedder.token_embedder_tokens.weight
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - Following parameters are Tunable (with gradient):
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_ih_l0
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_hh_l0
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_ih_l0
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_hh_l0
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_ih_l0_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_hh_l0_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_ih_l0_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_hh_l0_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_ih_l1
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_hh_l1
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_ih_l1
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_hh_l1
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_ih_l1_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.weight_hh_l1_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_ih_l1_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - encoder._module.bias_hh_l1_reverse
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - tag_projection_layer._module.weight
2019-02-07 19:10:12,478 - INFO - allennlp.commands.train - tag_projection_layer._module.bias
2019-02-07 19:10:12,478 - INFO - allennlp.common.params - trainer.type = default
2019-02-07 19:10:12,478 - INFO - allennlp.common.params - trainer.patience = 5
2019-02-07 19:10:12,478 - INFO - allennlp.common.params - trainer.validation_metric = -loss
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - trainer.shuffle = True
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - trainer.num_epochs = 40
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - trainer.cuda_device = -1
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - trainer.grad_norm = None
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - trainer.grad_clipping = None
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - trainer.optimizer = adam
2019-02-07 19:10:12,479 - INFO - allennlp.common.params - parameter_groups = None
2019-02-07 19:10:12,479 - INFO - allennlp.training.optimizers - Number of trainable parameters: 101802
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: 
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 20
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.model_save_interval = None
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.summary_interval = 100
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.histogram_interval = None
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.should_log_learning_rate = False
2019-02-07 19:10:12,480 - INFO - allennlp.common.params - trainer.log_batch_size_period = None
2019-02-07 19:10:12,482 - INFO - allennlp.common.params - evaluate_on_test = False
2019-02-07 19:10:12,482 - INFO - allennlp.training.trainer - Beginning training.
2019-02-07 19:10:12,482 - INFO - allennlp.training.trainer - Epoch 0/39
2019-02-07 19:10:12,482 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 279.604
2019-02-07 19:10:12,489 - INFO - allennlp.training.trainer - Training
>>> {'tokens': tensor([[  7, 427, 243,  ...,   0,   0,   0],
        [260,   7, 109,  ...,   0,   0,   0],
        [ 33,  10, 722,  ...,   0,   0,   0],
        ...,
        [ 39, 139,   3,  ...,   0,   0,   0],
        [153, 111, 772,  ...,   0,   0,   0],
        [  2,  82,   4,  ...,   0,   0,   0]])}
>>> >>> >>> >>> >>> >>> >>> >>> >>> >>> {'tokens': tensor([[  7, 427, 243,  ...,   0,   0,   0],
        [260,   7, 109,  ...,   0,   0,   0],
        [ 33,  10, 722,  ...,   0,   0,   0],
        ...,
        [ 39, 139,   3,  ...,   0,   0,   0],
        [153, 111, 772,  ...,   0,   0,   0],
        [  2,  82,   4,  ...,   0,   0,   0]])}
>>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> tensor([[[-0.0784,  0.0329,  0.0730,  ..., -0.0607, -0.0069, -0.0677],
         [-0.0454,  0.0952,  0.0828,  ..., -0.0290, -0.0150, -0.0547],
         [-0.1106,  0.1345,  0.1496,  ..., -0.0135,  0.0206, -0.0179],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0008,  0.0334,  0.0501,  ..., -0.0121,  0.0110, -0.0410],
         [-0.0214,  0.0466,  0.0614,  ..., -0.0594,  0.0015, -0.0696],
         [-0.0667,  0.0584,  0.1107,  ...,  0.0169,  0.0074, -0.0740],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0502,  0.0732,  0.0344,  ...,  0.0053,  0.0206,  0.0038],
         [-0.1048,  0.0584,  0.0518,  ..., -0.0653,  0.0111,  0.0098],
         [-0.1487,  0.0508,  0.1079,  ..., -0.0381,  0.0062,  0.0025],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        ...,

        [[-0.0452,  0.0486,  0.0715,  ...,  0.0018, -0.0956, -0.0947],
         [-0.0341,  0.0923,  0.1080,  ..., -0.0021, -0.0568, -0.0989],
         [-0.0809,  0.1119,  0.1181,  ...,  0.0352, -0.0565, -0.0802],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0918,  0.0477,  0.1200,  ..., -0.0928,  0.0195, -0.0806],
         [-0.0554,  0.0633,  0.1198,  ..., -0.0608,  0.0154, -0.1080],
         [-0.0454,  0.0771,  0.1382,  ..., -0.0843, -0.0429, -0.0815],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],

        [[-0.0845,  0.0506,  0.0809,  ..., -0.0075, -0.0256, -0.1077],
         [-0.1086,  0.0924,  0.1093,  ..., -0.0027, -0.0040, -0.1132],
         [-0.1238,  0.1110,  0.1439,  ..., -0.0110, -0.0536, -0.0819],
         ...,
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],
       grad_fn=<IndexSelectBackward>)
>>> torch.Size([32, 59, 100])
>>> >>> >>> ... ... >>> >>> >>> {'tokens': tensor([[  7, 427, 243,  ...,   0,   0,   0],
        [260,   7, 109,  ...,   0,   0,   0],
        [ 33,  10, 722,  ...,   0,   0,   0],
        ...,
        [ 39, 139,   3,  ...,   0,   0,   0],
        [153, 111, 772,  ...,   0,   0,   0],
        [  2,  82,   4,  ...,   0,   0,   0]])}
>>> torch.Size([32, 59])
>>> torch.Size([32, 59, 50])
>>> torch.Size([59, 50])
>>> {'tokens': tensor([[  7, 427, 243,  ...,   0,   0,   0],
        [260,   7, 109,  ...,   0,   0,   0],
        [ 33,  10, 722,  ...,   0,   0,   0],
        ...,
        [ 39, 139,   3,  ...,   0,   0,   0],
        [153, 111, 772,  ...,   0,   0,   0],
        [  2,  82,   4,  ...,   0,   0,   0]])}
>>> tensor([[ 0.3304,  0.2500, -0.6087,  ..., -0.5070, -0.0273, -0.5329],
        [-0.0286,  0.3199, -0.9672,  ..., -0.3377, -0.1713, -0.4997],
        [ 0.9184,  0.6531,  0.0649,  ...,  0.5010, -0.3749,  0.0484],
        ...,
        [-0.6248, -1.3540,  0.4286,  ..., -0.1540,  0.1758,  0.0530],
        [-0.6248, -1.3540,  0.4286,  ..., -0.1540,  0.1758,  0.0530],
        [-0.6248, -1.3540,  0.4286,  ..., -0.1540,  0.1758,  0.0530]])
>>> tensor([ 0.3304,  0.2500, -0.6087,  0.1092,  0.0364,  0.1510, -0.5508, -0.0742,
        -0.0923, -0.3282,  0.0960, -0.8227, -0.3672, -0.6701,  0.4291,  0.0165,
        -0.2357,  0.1286, -1.0953,  0.4333,  0.5707, -0.1036,  0.2042,  0.0783,
        -0.4279, -1.7984, -0.2786,  0.1195, -0.1269,  0.0317,  3.8631, -0.1779,
        -0.0824, -0.6270,  0.2650, -0.0572, -0.0735,  0.4610,  0.3086,  0.1250,
        -0.4861, -0.0080,  0.0312, -0.3658, -0.4270,  0.4216, -0.1167, -0.5070,
        -0.0273, -0.5329])
>>> tensor([-0.0286,  0.3199, -0.9672, -0.0954, -0.2694,  0.1161, -0.2311, -1.1441,
        -0.7094,  0.4596,  0.4191, -0.0053,  0.4401,  0.2262, -0.4684,  0.2065,
        -0.0618,  0.9988, -0.3006,  0.1329,  0.1197, -0.2330, -1.0298,  0.4215,
         0.8017, -1.3953, -1.7382, -0.4689,  0.0878, -0.1480,  3.0259, -0.7440,
         0.1987, -0.8472,  0.2107,  0.4918, -0.2164,  0.8015, -0.7937,  0.7776,
         0.0566, -0.1005,  0.7877,  0.2175,  0.0350,  1.1580,  0.1606, -0.3377,
        -0.1713, -0.4997])
>>> tensor([ 3.8336e-01, -9.5871e-02,  1.2229e-01, -5.1625e-01,  3.4910e-01,
         1.7050e-01, -5.5374e-01, -1.7357e-03, -4.7808e-01,  4.3859e-01,
        -2.5184e-01,  1.6347e-01, -2.2387e-01, -1.7963e-02,  8.0635e-01,
         4.2586e-01,  1.4927e-01, -1.9596e-01, -9.7068e-02, -9.7429e-01,
        -3.2088e-01,  1.5314e-01,  2.6854e-01, -1.1203e-02,  3.8486e-01,
        -2.0684e+00, -7.2715e-01,  3.1925e-01,  7.8713e-01, -2.2962e-01,
         3.5621e+00,  5.3464e-01, -1.3249e-02, -5.3812e-01, -1.7443e-02,
        -1.4833e-01, -2.0759e-01,  3.9231e-01, -3.0361e-01, -4.4828e-01,
        -4.8433e-01,  3.9363e-01,  1.0970e-01,  7.2701e-01,  1.0325e-01,
         6.0643e-02, -2.6806e-01, -5.6359e-02, -3.4692e-01,  1.3303e-01])
>>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> >>> tensor([-0.0286,  0.3199, -0.9672, -0.0954, -0.2694,  0.1161, -0.2311, -1.1441,
        -0.7094,  0.4596,  0.4191, -0.0053,  0.4401,  0.2262, -0.4684,  0.2065,
        -0.0618,  0.9988, -0.3006,  0.1329,  0.1197, -0.2330, -1.0298,  0.4215,
         0.8017, -1.3953, -1.7382, -0.4689,  0.0878, -0.1480,  3.0259, -0.7440,
         0.1987, -0.8472,  0.2107,  0.4918, -0.2164,  0.8015, -0.7937,  0.7776,
         0.0566, -0.1005,  0.7877,  0.2175,  0.0350,  1.1580,  0.1606, -0.3377,
        -0.1713, -0.4997])
>>> >>> >>> >>> >>> 